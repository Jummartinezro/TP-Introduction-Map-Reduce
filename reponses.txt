--------- 1.1 Exécution locale
1 - 
 Map input records : nombre de lignes dans le fichier en entrée
 Map output records : nombre total des occurences calculées au cours du Map
2 -
Map output records et Reduce input records :
Le résultats de la phase du Map est l'entrée de la phase du Reduce
3 -
Reduce input groups : c'est le nombre de groupes trouvés lors de la combinaison des clés resultant de la phase Map


--------- 1.2 Premier contact avec HDFS
quel est le chemin, dans HDFS, vers votre répertoire personel ?
/user/elhaddam
/user/martijua


--------- 1.3 - Exécution sur le cluster
Le compteur de nombre de splits correspand au nombre de découpes du fichier en entrée et qui est repartie sur les jobs.

--------- 1.4 - Combiner et nombre de reducers
1. Quelle est la diérence entre le répertoire de résultats obtenu ici, et celui de la partie 1.3 ?
Pourquoi ?
La différence est au niveau de "Launched reduce tasks" 
-Launched reduce tasks=3 pour cette execution
-Launched reduce tasks=1 pour la première execution
avec une différence sur les access au blocks
Data-local map tasks=3 contre Data-local map tasks=4
Rack-local map tasks=2 contre Rack-local map tasks=1 

2. Quels compteurs permettent de vérier que le combiner a fonctionné ?
C'est les compteurs :
Combine input records=421739
Combine output records=85301 

3. Quels compteurs permettent d'estimer le gain eectivement apporté par le combiner ? Com-
parez aux valeurs obtenues sans combiner pour justier votre réponse.

C'est le compteur :Spilled Records
sans combiner : 843478
avec combiner : 170602

le faite d'utiliser le combiner reduit les resultats à traités par le Reduce et du coups il reduit le nombre de donnée ecrits sur disque lorsque la taille de données dépasse la taille du buffer clé/valeur.

-------------- 2 - Top-tags Flickr par pays, avec tri en mémoire -------------------------

2.1 Pour pouvoir utiliser un combiner, quel devrait être le type des données intermédiaires ? Donnez le type sémantique (que représentent ces clés-valeurs ?) et le type Java.

Au debut de l’expérience, le combiner aura comme entrée la même sortie que celle du mapper. C’est-à-dire une clé <Text,Text> représentant le nom du pays et un tag associé. Comme sortie il devrait avoir aussi la même sortie reçue par le reducer: une clé <Text,StringAndInt> représentant le nom du pays et le tag avec le nombre d’occurrences.

2.2 Dans le reducer, nous avons une structure en mémoire dont la taille dépend du nombre de tags distincts : on ne le connaît pas a priori, et il y en a potentiellement beaucoup. Est-ce un problème ?

Après l’expérience, nous avons nous rendu compte que s’il y a beaucoup de données, alors Hadoop va utiliser le combiner. Par contre, quand il y en a pas beaucoup, Hadoop ne l’utilise pas. Comme la sortie du combiner et la sortie du reducer ne sont pas les mêmes, alors il y aura un problème.

Pour cette raison la sortie du mapper et la sortie du reducer devront être les mêmes. Après l’expérience, nous avons les types d’entrée-sortie suivants:

Mapper:		<LongWritable, Text, Text,StringAndInt>
Combiner: 	<Text, StringAndInt, Text,StringAndInt>
Reducer: 	<Text, StringAndInt, Text,StringAndInt>

-------------- 3 Top-tags Flickr par pays, avec tri par Hadoop -------------------------

Question préliminaire : spéci􏰅fiez les 2 jobs nécessaires (le second va utiliser le résultat du premier), en précisant le type des clés/valeurs en entrée, sortie et intérmédiaires. Pour le second, spécifi􏰅ez aussi les comparateurs à utiliser pour le tri et pour le découpage en groupes.

Premier Job:
	Mapper:		<LongWritable, Text, Text,Text> 
			Entrée: Le Text
			Sortie: (Pays, Tag) compté 1 fois

	Reducer:	<Text, Text, Text,Int>
			Entrée: (Pays,Tag), 1 fois
			Sortie: (Pays,Tag), Nb fois
Deuxième Job:		
	Mapper:		<Text, Int, StringAndInt, Text> 
			Entrée: (Pays,Tag), Nb fois
			Sortie: (Pays,Nb fois), Tag

	Intermediaire:	<StringAndInt, Text, StringAndInt, Text> 
			Entrée: (Pays,Nb fois), Tag
			Sortie: (Pays,Nb fois), Tag (Mais organisé par groupes (Pays,Nb fois))
			(Le comparateur est le clé (Pays,Nb fois))

	Reducer:	<StringAndInt, Text, StringAndInt,Text>
			Entrée: (Pays,Nb fois), Tag
			Sortie: (Pays,Nb fois), Tag

3.1. Un avantage de cette méthode est que le tri est réalisé sur le disque dur des data nodes, plutôt qu'en mémoire : on peut donc trier des quantités de données plus importantes. Dans notre application, cette méthode présente un autre avantage pour la fonction reduce 􏰅finale; lequel ?



3.1.2. S’il existe des tags classés ex aequo dans le top-K d'un pays, a-t-on la garantie d'obtenir toujours le même résultat d'une éxécution à l'autre ? Pourquoi ?



3.2.1. Le 􏰅chier complet pèse 44.4 Go, et nous utilisons des blocs de 64Mo. Avec combien de machines le premier map sera le plus rapide possible ?


3.2.2. Une fois que nous utilisons ce partitioner, le deuxième job génère-t-il des transferts de données sur le réseau ? Pourquoi ?


3.2.3. En pratique : si les deux jobs utilisent job.setNumReduceTasks(3), d'après les compteurs que se passe-t-il ?


